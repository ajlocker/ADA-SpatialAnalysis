---
title: "Spatial Analysis"
author: "Julia Earle, Fatemeh Ghaheri, and Angelina Locker"
date: "May 4, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Preliminaries
Load and install the following packages: {tmap}, {leaflet}, {rgdal}, {rgeos}, {sp}, {raster}, {adehabitatHR}, {devtools}, {rayshader}, {mapmate}

##Objectives
The objective of this module is to demonstrate some of the capabilities R has for handling spatial data. We will provide you with a brief overview of the various formats of spatial data, explain how to import spatial data into R, and provide a few examples of spatial data manipulation.  


Before we begin with the data, you'll need to link up the the GitHub repository. Create a New Project --> Version Control --> Git:
https://github.com/ajlocker/ADA-SpatialAnalysis.git

This will allow you to access all the necessary files for this module. 


#Shapefile
Shapefile is a data storage format where you can store different types of geographic information and geospatial data such as location, shape, line and points and their various details such as names etc. The information in a shapefile is actually a big set of files with large data in them combined within one shapefile. Like a csv file shapefile (shp file for short) also has different set of details and files where large data is stored that can be seen in columns when loaded to R. Many GIS applications and softwares such as ArcGIS use shp file for their visualizations, mapping and analyses. But since they are not usually free it’s a great feature that R has as a free tool.

#Point data vs. raster data
Point data represent distinct and separate points and are commonly defined by their geographic coordination and can be located within a polygon, for example location of a bridge and an archaeological site can be shown as a point.

Raster data or “gridded” data represents surfaces and are saved as pixels where each pixel represents a surface.  

So now that we know the definition of the above terms we know that I mentioned them because they are among the most important features in geospatial visualization.

##Loading GIS Data into R

First we will show how you can load GIS data into R, and how you can manipulate it to create basic maps.

##Density Mapping

In the density mapping section, we will learn how to visualize concentrations of point data. Specifically, we will use kernel density estimation to calculate where the highest concentrations of point data are located in our data set.


```{R}
library(tmap)
library(leaflet)
library(rgdal)
library(rgeos)
library(sp)
library(raster)
library(adehabitatHR)

```

Before we start, please make sure that you set your working directory, and download all the required files into your working directory.

```{R}
#Using a one line code you can load shapefile into R and read it: #m <- readOGR(".", "Zipped_SPA_Shape_file") 
#The other format of code you can use for this purpose is m1 <- readOGR("shp/Zipped_SPA_Shape_file.shp",layer="Zipped_SPA_Shape_file"), but i found this format a little problematic and confusing, and it also gives erros if you dont use it carefully, so i recomment the first code
#The “.” Is the source and the "Zipped_SPA_Shape_file" is the layer which is the name of the file without the suffixes. 
#read the shapefile using the bellow code

# Using a one line code you can load shapefile into R and read it: m <- readOGR(".", "Zipped_SPA_Shape_file") 
# The other format of code you can use for this purpose is m1 <- readOGR("shp/Zipped_SPA_Shape_file.shp",layer="Zipped_SPA_Shape_file"), but i found this format a little problematic and confusing, and it also gives erros if you dont use it carefully, so i recomment the first code
# The “.” Is the source and the "Zipped_SPA_Shape_file" is the layer which is the name of the file without the suffixes. 
# read the shapefile using the bellow code

mydata1 <-readOGR(".", "Zipped_SPA_Shape_file")
mydata1
head(mydata1)
```

```{R}
# Plot the data into a map
plot(mydata1)
```

```{R}
# Creates a choropleth map of our any( in this case SQ_MILES) variable
# With a choropleth map you can show statistical data with different specifications such as color and symbols shading on the map
## qtm function: allows you draw a thematic map. when you add fill=   to the code then you specify how you want your details to be shown on the map. but without fill=  you will just get a map without any specification.
m <- qtm(mydata1, fill = "SQ_MILES")
m
```

```{R}
#Removes the black borders
##here with tm_shape fucntion we specify a spatial data object and then we can add various elements to it.
tm_shape(mydata1) + tm_fill("SQ_MILES")
```
```{R}
#Add borders
#Alpha is the transparency of the borders 
##using the function tm_borders we are able to add borders.
tm_shape(mydata1) + tm_fill("SQ_MILES") + tm_borders(alpha=.9)
```

```{R}
#Adding a north arrow
tm_shape(mydata1) + tm_fill("SQ_MILES") + tm_compass()
```

```{R}
#Giving a title to the map
tm_shape(mydata1) + tm_fill("SQ_MILES", palette = "Reds",
style = "quantile", title = "% with a SQ_MILES") +
tm_borders(alpha=.4) +
tm_compass() +
tm_layout(title = "Camden, Arizona", legend.text.size = 1.1,
legend.title.size = 1.4, legend.position = c("right", "top"), frame = FALSE)
```

## ## Mapping densities

Density mapping is a means of visualizing concentrations of spatial phenomena. Point data are most often used for density mapping, but lines can also be used. If you have a high volume of point data distributed across a large area, it may be difficult to understand the distribution of points. Density maps can simplify this data by showing where points are concentrated.

Density mapping can be incredibly useful when you have a large data set representing isolated phenomena over a continuous surface (e.g., artifacts, populations). As such, density mapping would not be useful for representing small data sets, or data that represents continuous phenomena (e.g., elevation or temperature). With spatial data visualization it is important to understand first what kind of data you have, and what you want to represent or analyze.

```{R}
# Download data from https://github.com/ajlocker/ADA-SpatialAnalysis/blob/master/DEPARTAMENTO.shp
# Remember, the files must be placed into your working directory

# Loading and plotting shapefile (Peru and its departamentos)
DEPARTAMENTO <- readOGR(".", "DEPARTAMENTO")
# Setting the coordinate system for the shapefile
proj4string(DEPARTAMENTO) <- CRS("+init=EPSG:32718")
plot(DEPARTAMENTO)
```
This shapefile represents a map of Peru with the departments outlined. We will plot the distribution of recorded archaeological sites in Peru onto this map.

```{R}
#Download data from https://github.com/ajlocker/ADA-SpatialAnalysis/blob/master/SitioArqueologico.shp

#Loading and plotting point data (recorded archaeological sites in Peru)
SitioArqueologico <- readOGR(".", "SitioArqueologico")
# Setting the coordinate system for the point data
proj4string(SitioArqueologico) <- CRS("+init=EPSG:32718")
plot(SitioArqueologico)
```

These points represent the distribution of recorded archaeological sites in Peru. Here we see that we have a very high density of points and it is difficult to distinguish where the higher concentrations of points are located at this resolution.

## Kernel density estimation

To begin, we will perform kernel density estimation on the point data. Kernel density estimation uses a moving quadrant to calculate the density of points for each area within a given threshold.

For kernel density estimation, be sure you have loaded the package "adehabitatHR" into your library. Here we use the kernelUD function. UD refers to Utilization Distribution, which is a bivariate function. Other functions are available to calculate kernel density in this package.

```{R}
kde.output <- kernelUD(SitioArqueologico, h="href", grid = 1000)
# Grid determines how many pixels will be used for the map. Using a smaller number of pixels will run faster, but will not be as detailed. Grid value should be selected according to how much detail you need to represent.
plot(kde.output)
```

If we want to map this density estimate, we need to first convert it to a raster.

```{R}
# Convert to raster
kde <- raster(kde.output)
# Map raster
tm_shape(kde) + tm_raster("ud")
```

The value in the legend refers to the value calculated from the UD function. A higher UD corresponds to a higher density here.

These maps are much more useful than the original point data to tell us about where most archaeological sites have been recorded. However, without the boundaries of the country of Peru and its departments, it is difficult to determine where exactly the highest densities of archaeological sites are located. To this end, we will plot our kernel density estimates on the original shapefile. 

We cannot map the kernel density directly onto the shapefiles, so we must create 'catchment areas' at 75%, 50%, and 25% intervals.

```{R}
# Mask the raster by the output area polygon
masked_kde <- mask(kde, SitioArqueologico)
# Catchment boundaries for kernel density estimates
# Top 75% highest density
range75 <- getverticeshr(kde.output, percent = 75)
plot(range75)
```

```{R}
#Top 50% highest density
range50 <- getverticeshr(kde.output, percent = 50)
plot(range50)
```

```{R}
#Top 25% highest density
range25 <- getverticeshr(kde.output, percent = 25)
plot(range25)
```

With this code we have generated our catchment areas, which we can map onto the shape file and point data. Next, we will map the point data and the catchment boundaries onto our original map of Peru with its departments.

```{R}
tm_shape(DEPARTAMENTO) + tm_fill(col = "#f0f0f0") + tm_borders(alpha=.8, col = "white") + tm_shape(SitioArqueologico) + tm_dots(col = "blue") +
tm_shape(range75) + tm_borders(alpha=.7, col = "#fb6a4a", lwd = 2) + tm_fill(alpha=.1, col = "#fb6a4a") +
tm_shape(range50) + tm_borders(alpha=.7, col = "#de2d26", lwd = 2) +
tm_fill(alpha=.1, col = "#de2d26") +
tm_shape(range25) + tm_borders(alpha=.7, col = "#a50f15", lwd = 2) +
tm_fill(alpha=.1, col = "#a50f15") + tm_layout(frame = FALSE)
```

To review, with the code outlined above we have calculated kernel density esitmates and mapped these densities onto shapefiles and point data in order to understand where the highest densities of points are located on our map.

```{R}
tm_shape(DEPARTAMENTO) + tm_fill(col = "#f0f0f0") + tm_borders(alpha=.8, col = "white") + tm_shape(SitioArqueologico) + tm_dots(col = "blue") +
tm_shape(range75) + tm_borders(alpha=.7, col = "#fb6a4a", lwd = 2) + tm_fill(alpha=.1, col = "#fb6a4a") +
tm_shape(range50) + tm_borders(alpha=.7, col = "#de2d26", lwd = 2) +
tm_fill(alpha=.1, col = "#de2d26") +
tm_shape(range25) + tm_borders(alpha=.7, col = "#a50f15", lwd = 2) +
tm_fill(alpha=.1, col = "#a50f15") + tm_layout(frame = FALSE)
```


#LiDAR Data and R

**Li**ght **D**etection **A**nd **R**anging **(LiDAR)** is very useful survey method that uses pulses of laser light to measure reflected distance between a given sensor (typically on the bottom of an airplane) and a survey area. 

![LiDAR Illustration](Lidar-Graphic1.jpg)

This becomes exceptionally useful when your research area is in the middle of a neo-tropical jungle. 3D images and maps can be created based on the differences in response time, where objects that are higher in elevation are returned more quickly than objects lower in elevation.  

![Caana Temple, Caracol](Jungle with Pyramid.jpg)

You can see that the forest canopy is fairly dense around the site. LiDAR essentially allows us to complete an aerial survey and remove the canopy layer to see what is happening below the tree line.  

![Caracol map compared to LiDAR imagery](Caracol LiDAR Map.jpg)

##Using LiDAR data to create Digital Elevation Models in R

The first thing we will need to do is set up a working directory for the files.

```{r}
# Load needed packages
library(raster)

# set working directory to data folder
# setwd("C:/Users/ajlocker/Desktop/R Geospatial")
```

You'll need to go to download the following files and save them inside your working directory.

https://github.com/ajlocker/ADA-SpatialAnalysis/blob/master/ATXLiDAR/demTIN.tif

https://github.com/ajlocker/ADA-SpatialAnalysis/blob/master/ATXLiDAR/hsTIN.tif

https://github.com/ajlocker/ADA-SpatialAnalysis/blob/master/ATXLiDAR/demlocal.tif


Next, we are going to load in the data by assigning them to be rasters. The DEM file is a Digital Elevation Model, which tells R elevation values from the scanned LiDAR data. The hs file is a Hillslope image.  

```{r}
# assign raster to object
dem <- raster("C:/Users/ajlocker/Desktop/R Geospatial/ATX/demTIN.tif")

# view info about the raster.
dem
```

Plot the Digital Elevation Model
```{r}
plot(dem, main="Digital Elevation Model \n Austin, Texas")
```


So here, we can see basic elevation differences from the surveyed area, but it doesn't show us much detail from the LiDAR imaging. We can examine this data much more effectively by plotting it on top of the Hillshade file, which will essentially add depth to the map. 

```{r}
hs <- raster("C:/Users/ajlocker/Desktop/R Geospatial/ATX/hsTIN.tif")

# view info about the raster.Pay attention to the extent portion. We'll be manipulating this data in a bit. 
hs
```

```{r}
# plot hillshade file
plot(hs,
        col=grey(1:100/100),  # creates a gray scale coloring for the hillshade data
        legend=FALSE,         # removes any legend
        main="LiDAR Survey Austin, Tx", #sets the title of the image
        axes=TRUE)          # keeps axes titles so we can see the coresponding extent data (xmin, xmax, ymin, ymax)

#then plot the DEM ontop of the hillshade
plot(dem, 
        axes=T,
        alpha=0.5,   # transparency of the object (0=transparent, 1=not transparent)
        add=T)  # add=TRUE (or T); this will add the chm plot on top of the plot we coded immediately above
```

#Creating an Interactive Map

We're going to use the {leaflet} package to plot our lidar data onto an existing map. We can then zoom in and out to see where our data are geographically. 


```{r}
library(leaflet)
hs <- raster("C:/Users/ajlocker/Desktop/R Geospatial/ATX/hsTIN.tif")
pal <- colorNumeric(c("#0C2C84", "#41B6C4", "#FFFFCC"), values(hs),
  na.color = "transparent")

leaflet() %>% addTiles() %>%
  addRasterImage(hs, colors = pal, opacity = 0.8) %>%
  addLegend(pal = pal, values = values(hs),
    title = "Elevation")
```
`
#Creating 3D Maps
There is a great package called Rayshader, where you can mainpulate raster images in some really cool ways. 

```{r}
# To install the latest version from Github:
library(devtools)
devtools::install_github("tylermorganwall/rayshader")
```

We're going to use a raster file of just elevation points to generate this map. First, I'm going to have you crop the data to make the map a bit more dramatic looking. We're going to crop the data to a specific quandrant. To do this, we'll use the {raster} package.

```{r}
cp <- as(extent(620669, 620900, 3348680, 3349000), 'SpatialPolygons')
crs(cp) <- crs(dem)
atxcp <- crop(dem, cp)
atxcp
plot(atxcp)
```

Now that our raster is cropped, we will create our 3D maps. 
```{r}
library(rayshader)

#First we need to convert our cropped raster file to a matrix:
atxmat = matrix(raster::extract(atxcp,raster::extent(atxcp),buffer=1000),
               nrow=ncol(atxcp),ncol=nrow(atxcp))

#Then we are going to plot with a slight shadow
atxmat %>%
  sphere_shade(texture = "bw") %>%
  add_shadow(ray_shade(atxmat,lambert = TRUE), 0.7) %>%
    plot_map()
```

```{r}
#adds ambient shading to the map
ambmat = ambient_shade(atxmat)

#This will generate a 3D interactive map. It should generate a pop-up window
atxmat %>%
  sphere_shade(texture = "bw") %>%
  add_water(detect_water(atxmat), color="bw") %>%
  add_shadow(ray_shade(atxmat,zscale=3,maxsearch = 300),0.5) %>%
  add_shadow(ambmat,0.5) %>%
  plot_3d(atxmat,zscale=10,fov=0,theta=135,zoom=0.75,phi=45, windowsize = c(1000,800))
render_snapshot()
```

You could then take this 3D generated plot and print it on a 3D printer using the command save_3dprint("atx.stl")


##Concept Review
Spatial Data has sevaral formats and each of them can be read in R. 

R has the ability to read and manipulate LiDAR data. 

You have the ability to generate interative and 3D maps/models with packages like {leaflet} and {rayshader}.

###References
Rayshader.com

https://www.neonscience.org/neon-lidar-rasters-workshop-20150514

https://gis.stackexchange.com/questions/229356/crop-a-raster-file-in-r
